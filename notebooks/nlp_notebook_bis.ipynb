{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced NLP Pipeline for Global Question Extraction (Bis)\n",
    "\n",
    "This notebook builds on our previous NLP workflows to produce a refined, multilingual mapping of individual survey items to overarching “global” themes. We:\n",
    "\n",
    "1. **Set up and load data**  \n",
    "   - Import required NLP, data-wrangling, and clustering libraries  \n",
    "   - Load the master list of global questions and all sheets from the CodeBook  \n",
    "\n",
    "2. **Clean and merge question metadata**  \n",
    "   - Apply year-specific filters to legacy vs. recent survey questions  \n",
    "   - Normalize `num_question` codes and drop empty entries  \n",
    "   - Concatenate all sheets into one unified DataFrame  \n",
    "\n",
    "3. **Preprocess text for NLP**  \n",
    "   - Lowercase, tokenize, remove stop-words and punctuation  \n",
    "   - Lemmatize tokens to standardize word forms  \n",
    "\n",
    "4. **Compute semantic similarities**  \n",
    "   - Generate embeddings using both BERT and DSBSAN transformer models  \n",
    "   - Vectorize labels with TF-IDF and compute cosine similarity matrices  \n",
    "   - Compare transformer vs. TF-IDF approaches to choose the optimal similarity metric  \n",
    "\n",
    "5. **Cluster into global themes**  \n",
    "   - Use DBSCAN on similarity distances to group related questions  \n",
    "   - Select the top global themes based on a similarity threshold  \n",
    "\n",
    "6. **Extract and compile global question table**  \n",
    "   - Build a final DataFrame mapping each question code back to its global parent  \n",
    "   - Transpose and align label options for consistency  \n",
    "\n",
    "7. **Translate global labels**  \n",
    "   - Attempt LibreTranslate API (not available locally)  \n",
    "   - Fallback to BERT-based translation models for high-quality English labels  \n",
    "   - Note unsupported languages (e.g., Romanche) where translations aren’t available  \n",
    "\n",
    "8. **Export final results**  \n",
    "   - Save the enriched, multilingual `global_questions_nlp_bis.csv` for downstream analysis and reporting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests\n",
    "import time\n",
    "from transformers import pipeline\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, MarianMTModel, MarianTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files \n",
    "question_globales_path = 'data/QuestionGlobales.xlsx'\n",
    "extraction_codebook_path = 'data/Extraction CodeBook - 3. Cleaned.xlsx'\n",
    "gsb_path = 'data/GSB 2023_V1.xlsx'\n",
    "\n",
    "# Read the global questions file\n",
    "df_globales = pd.read_excel(question_globales_path)\n",
    "\n",
    "# Read all sheets from the codebook file\n",
    "sheets_codebook = pd.read_excel(extraction_codebook_path, sheet_name=None)\n",
    "\n",
    "# answers dataframe\n",
    "gsb_df = pd.read_excel(gsb_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of data manipulation before NLP preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for each year\n",
    "years = ['1988', '1994', '1998', '2005', '2009', '2017', '2023']\n",
    "dataframes = {}\n",
    "\n",
    "# choose the columns to keep\n",
    "columns_to_keep = ['code', 'num_question', 'year', 'label', 'type', 'format']\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    if year in sheets_codebook:  # ensure the sheet exists\n",
    "        # each sheet is a DataFrame\n",
    "        df = sheets_codebook[year]\n",
    "        # keep only the columns we need (defined in columns_to_keep)\n",
    "        dataframes[f'main_questions_{year}'] = df[columns_to_keep]\n",
    "    else:\n",
    "        print(f\"Sheet for year {year} not found.\")\n",
    "\n",
    "# define the dataframes\n",
    "main_questions_1988 = dataframes.get('main_questions_1988', pd.DataFrame())\n",
    "main_questions_1994 = dataframes.get('main_questions_1994', pd.DataFrame())\n",
    "main_questions_1998 = dataframes.get('main_questions_1998', pd.DataFrame())\n",
    "main_questions_2005 = dataframes.get('main_questions_2005', pd.DataFrame())\n",
    "main_questions_2009 = dataframes.get('main_questions_2009', pd.DataFrame())\n",
    "main_questions_2017 = dataframes.get('main_questions_2017', pd.DataFrame())\n",
    "main_questions_2023 = dataframes.get('main_questions_2023', pd.DataFrame())\n",
    "\n",
    "main_questions_2023.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheet filtering for num_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tailor made data cleaning for the old years sheets \n",
    "def filter_num_question_old(df):\n",
    "    # delete rows with empty num_question\n",
    "    df = df[df['num_question'].notna()].copy()  \n",
    "\n",
    "    # convert num_question to string\n",
    "    df.loc[:, 'num_question'] = df['num_question'].astype(str)  \n",
    "\n",
    "    # some tailor made filtering --> keep only rows with num_question that are digits or contain 'a' or 'A' \n",
    "    df = df[df['num_question'].str.match(r'^\\d+$|.*[aA].*')]\n",
    "    \n",
    "    # keep only the first occurence of each num_question\n",
    "    df = df.drop_duplicates(subset='num_question', keep='first')\n",
    "\n",
    "    # delete the a or A from the num_question\n",
    "    df['num_question'] = df['num_question'].str.replace('a', '', regex=False).str.replace('A', '', regex=False)\n",
    "\n",
    "    # keep only the first occurence of each num_question\n",
    "    df = df.drop_duplicates(subset='num_question', keep='first')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tailor made data cleaning for the new years sheets \n",
    "def filter_num_question_new(df):\n",
    "    # delete rows with empty num_question\n",
    "    df = df[df['num_question'].notna()].copy()  \n",
    "    # keep only the first occurence of each num_question\n",
    "    df = df.drop_duplicates(subset='num_question', keep='first')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_questions_1988 = filter_num_question_old(main_questions_1988)\n",
    "main_questions_1994 = filter_num_question_old(main_questions_1994)\n",
    "main_questions_1998 = filter_num_question_old(main_questions_1998)\n",
    "main_questions_2005 = filter_num_question_old(main_questions_2005)\n",
    "main_questions_2009 = filter_num_question_old(main_questions_2009)\n",
    "main_questions_2017 = filter_num_question_new(main_questions_2017)\n",
    "main_questions_2017 = main_questions_2017[main_questions_2017['num_question'] != 'Q1']\n",
    "main_questions_2023 = filter_num_question_new(main_questions_2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_questions_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_merge = [\n",
    "    main_questions_1988,\n",
    "    main_questions_1994,\n",
    "    main_questions_1998,\n",
    "    main_questions_2005,\n",
    "    main_questions_2009,\n",
    "    main_questions_2017,\n",
    "    main_questions_2023\n",
    "]\n",
    "\n",
    "# merge all the dataframes\n",
    "merged_df = pd.concat(dataframes_to_merge, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. delete punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 3. delete extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 4. lemmatization + tokenization + remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop] \n",
    "    \n",
    "    return tokens  \n",
    "\n",
    "# apply the preprocess_text function to the 'label' column\n",
    "tqdm.pandas() \n",
    "merged_df['tokens'] = merged_df['label'].progress_apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['year'] == 2023]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pipeline: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer version: Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dbmdz/bert-base-german-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(labels):\n",
    "    embeddings = []\n",
    "    for label in tqdm(labels, desc='Generating embeddings'):\n",
    "        inputs = tokenizer(label, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()  # convert the list of tokens to a string\n",
    "\n",
    "# obtain the embeddings for the tokens\n",
    "embeddings = get_embeddings(labels)\n",
    "\n",
    "# use cosine similarity to calculate the similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# create a DataFrame from the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=labels, columns=labels)\n",
    "\n",
    "# set the threshold for similarity --> here 0.97 but can be changed\n",
    "threshold = 0.97\n",
    "\n",
    "similar_pairs = []\n",
    "\n",
    "# iterate over the similarity matrix to identify similar pairs\n",
    "for i in tqdm(range(len(labels)), desc='Identifying similar pairs'):\n",
    "    for j in range(i + 1, len(labels)):  \n",
    "        if similarity_matrix[i][j] >= threshold:\n",
    "            similar_pairs.append((labels[i], labels[j]))\n",
    "\n",
    "# create a new DataFrame to store the combined rows\n",
    "question_globale_new = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "for label1, label2 in similar_pairs:\n",
    "    # add a row for each pair of similar tokens\n",
    "    temp_df = merged_df[(merged_df['tokens'].apply(lambda x: ' '.join(x)) == label1) | \n",
    "                        (merged_df['tokens'].apply(lambda x: ' '.join(x)) == label2)]\n",
    "    combined_row = {\n",
    "        'id': id_counter,  # ad id for each combined row\n",
    "        'label': f\"{label1}; {label2}\",\n",
    "        'year': '; '.join(temp_df['year'].astype(str)),\n",
    "        'code': '; '.join(temp_df['code'])\n",
    "    }\n",
    "\n",
    "    question_globale_new = pd.concat([question_globale_new, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "\n",
    "    id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new[question_globale_new['id'] == 1].label.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer version: DSBSAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()\n",
    "\n",
    "# embedding model\n",
    "def get_embeddings(labels):\n",
    "    embeddings = []\n",
    "    for label in tqdm(labels, desc=\"embedding computation\", unit=\"label\"):\n",
    "        inputs = tokenizer(label, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy()  \n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# obtain unique labels\n",
    "labels = merged_df['label'].unique()\n",
    "\n",
    "# obtain embeddings for the labels\n",
    "embeddings = get_embeddings(labels)\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "# use cosine similarity to calculate the similarity matrix\n",
    "distance_matrix = cosine_distances(embeddings)\n",
    "\n",
    "# apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=3, metric='precomputed')  # use precomputed to pass the distance matrix\n",
    "clusters = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "# create a dictionary to map labels to indices\n",
    "label_to_index = {label: index for index, label in enumerate(merged_df['label'].unique())}\n",
    "\n",
    "# create a Series to store the clusters\n",
    "cluster_series = pd.Series(-1, index=merged_df.index)\n",
    "\n",
    "# assign the clusters to the labels\n",
    "for label, cluster_id in zip(labels, clusters):\n",
    "    index = label_to_index[label]\n",
    "    cluster_series.iloc[index] = cluster_id\n",
    "\n",
    "# add the clusters to the merged DataFrame\n",
    "merged_df['cluster'] = cluster_series\n",
    "\n",
    "# create a new DataFrame to store the combined rows\n",
    "question_globale_new = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "# fill the new DataFrame with the combined rows\n",
    "id_counter = 1\n",
    "for cluster_id in set(merged_df['cluster']):\n",
    "    if cluster_id != -1:  \n",
    "        temp_df = merged_df[merged_df['cluster'] == cluster_id]\n",
    "        if not temp_df.empty:\n",
    "            combined_row = {\n",
    "                'id': id_counter,\n",
    "                'label': '; '.join(temp_df['label']),\n",
    "                'year': '; '.join(temp_df['year'].astype(str)),\n",
    "                'code': '; '.join(temp_df['code'])\n",
    "            }\n",
    "            question_globale_new = pd.concat([question_globale_new, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "            id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new[question_globale_new['id'] == 1].label.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 global questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(years):\n",
    "    return len(years) != len(set(years))\n",
    "\n",
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()\n",
    "\n",
    "# create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(labels)\n",
    "\n",
    "# calculate the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "similarity_matrix[np.tril_indices_from(similarity_matrix)] = -1\n",
    "\n",
    "top_10_indices = np.dstack(np.unravel_index(np.argsort(similarity_matrix.ravel())[-10:], similarity_matrix.shape))[0]\n",
    "\n",
    "question_globale_new_top_10 = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "# Traiter les 10 paires ayant la plus grande similarité\n",
    "for i, j in top_10_indices:\n",
    "    label1 = labels[i]\n",
    "    label2 = labels[j]\n",
    "    \n",
    "    # Ajouter une ligne pour chaque paire de tokens similaires\n",
    "    temp_df = merged_df[(merged_df['tokens'].apply(lambda x: ' '.join(x)) == label1) | \n",
    "                        (merged_df['tokens'].apply(lambda x: ' '.join(x)) == label2)]\n",
    "    \n",
    "    combined_years = '; '.join(temp_df['year'].astype(str)).split('; ')\n",
    "\n",
    "    # Vérifier s'il y a des années dupliquées\n",
    "    if has_duplicates(combined_years):\n",
    "        continue  # Ignorer la ligne si des années dupliquées sont trouvées\n",
    "    \n",
    "    # Construire la ligne combinée\n",
    "    combined_row = {\n",
    "        'id': id_counter,  \n",
    "        'label': f\"{label1}; {label2}\",\n",
    "        'year': '; '.join(combined_years),\n",
    "        'code': '; '.join(temp_df['code'])\n",
    "    }\n",
    "\n",
    "    # Ajouter la ligne au DataFrame question_globale_new\n",
    "    question_globale_new_top_10 = pd.concat([question_globale_new_top_10, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "\n",
    "    id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.loc[question_globale_new_top_10['year'].str.contains('2023'), 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10[question_globale_new_top_10['id'] == 1].label.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question global selection based on threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(years):\n",
    "    return len(years) != len(set(years))\n",
    "\n",
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()\n",
    "\n",
    "# create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(labels)\n",
    "\n",
    "# calculate the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# create a DataFrame from the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=labels, columns=labels)\n",
    "\n",
    "# define the threshold for similarity, here 0.8 seems to be a good value but can be changed \n",
    "threshold = 0.6\n",
    "\n",
    "similar_pairs = []\n",
    "\n",
    "# identify similar pairs\n",
    "for i in tqdm(range(len(labels)), desc='Identifying similar pairs'):\n",
    "    for j in range(i + 1, len(labels)):\n",
    "        if similarity_matrix[i][j] >= threshold:\n",
    "            similar_pairs.append((labels[i], labels[j]))\n",
    "\n",
    "# create a new DataFrame to store the combined rows\n",
    "question_globale_new_threshold = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "for label1, label2 in similar_pairs:\n",
    "    # add a row for each pair of similar tokens\n",
    "    temp_df = merged_df[(merged_df['tokens'].apply(lambda x: ' '.join(x)) == label1) | \n",
    "                        (merged_df['tokens'].apply(lambda x: ' '.join(x)) == label2)]\n",
    "    \n",
    "    combined_years = '; '.join(temp_df['year'].astype(str)).split('; ')\n",
    "\n",
    "    if has_duplicates(combined_years):\n",
    "        continue\n",
    "    \n",
    "    combined_row = {\n",
    "        'id': id_counter,  \n",
    "        'label': f\"{label1}; {label2}\",\n",
    "        'year': '; '.join(combined_years),\n",
    "        'code': '; '.join(temp_df['code'])\n",
    "    }\n",
    "\n",
    "    question_globale_new_threshold = pd.concat([question_globale_new_threshold, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_threshold.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the results seems better --> use the TF-IDF version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV extraction of the global questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_glob_columns = [\n",
    "    'label', 'code_first_question','code_other_question','text_de', 'text_fr', 'text_it', 'text_ro', 'text_en',\n",
    "    'category_label', 'category_text_de', 'category_text_fr',\n",
    "    'category_text_it', 'category_text_ro', 'category_text_en',\n",
    "    'options_value', 'options_label'\n",
    "]\n",
    "\n",
    "code_to_token = {}\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    code = row['code']\n",
    "    tokens = row['tokens']\n",
    "\n",
    "    for token in tokens:\n",
    "        if not token.isdigit():  \n",
    "            if code not in code_to_token:  \n",
    "                code_to_token[code] = token\n",
    "            break\n",
    "\n",
    "\n",
    "print(code_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame to store the final results\n",
    "df_fin_top_10 = pd.DataFrame(columns=quest_glob_columns)\n",
    "\n",
    "# fill the new DataFrame with the combined rows\n",
    "for index, row in question_globale_new_top_10.iterrows():\n",
    "    codes_list = row['code'].split('; ')\n",
    "    gsb23_code = next((code for code in row['code'].split('; ') if code.startswith('GSB23_')), None)\n",
    "    code_first_question = gsb23_code if gsb23_code is not None else row['code'].split('; ')[0]  # use 'GSB23_' code or the first code\n",
    "    code_other_question = '; '.join([code for code in codes_list if code != code_first_question])\n",
    "    first_token = code_to_token.get(code_first_question, '')  # use the token corresponding to the first code\n",
    "\n",
    "    # Find the corresponding text_de in merged_df using code_first_question\n",
    "    label_from_merged = merged_df.loc[merged_df['code'] == code_first_question, 'label'].values\n",
    "    if len(label_from_merged) > 0:\n",
    "        # Remove any leading numbers and dots (e.g., \"52. \" or \"123. \")\n",
    "        cleaned_label = re.sub(r'^\\d+\\.\\s*', '', label_from_merged[0])\n",
    "        text_de_first_question = cleaned_label\n",
    "    else:\n",
    "        text_de_first_question = row['label'].split('; ')[0]\n",
    "\n",
    "    # create a new row with the required columns\n",
    "    new_row = {\n",
    "        'label': first_token,  # use the first code in 'label'\n",
    "        'code_first_question': code_first_question,  # put the first code in 'code_first_question'\n",
    "        'code_other_question': code_other_question,  # put the other codes in 'code_other_question'\n",
    "        'text_de': text_de_first_question,  # use the first label in 'text_de'\n",
    "        'text_fr': '',  \n",
    "        'text_it': '', \n",
    "        'text_ro': '', \n",
    "        'text_en': '',\n",
    "        'category_label': '',\n",
    "        'category_text_de': '',\n",
    "        'category_text_fr': '',\n",
    "        'category_text_it'  : '',\n",
    "        'category_text_ro': '',\n",
    "        'category_text_en': '',\n",
    "        'options_value': '',\n",
    "        'options_label': ''\n",
    "    }\n",
    "\n",
    "    # add the new row to the final DataFrame\n",
    "    df_fin_top_10 = pd.concat([df_fin_top_10, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "df_fin_top_10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsb_columns = gsb_df.columns\n",
    "\n",
    "# fill the 'options_value' column in df_fin\n",
    "for index, row in df_fin_top_10.iterrows():\n",
    "    # get the code of the first question\n",
    "    question_code = row['code_first_question']\n",
    "    \n",
    "    # check if the question code exists in the 'GSB 2023_V1' DataFrame\n",
    "    if question_code in gsb_columns:\n",
    "        # extract the unique values from the column\n",
    "        unique_values = gsb_df[question_code].dropna().unique()\n",
    "        \n",
    "        # if there are unique values, join them \n",
    "        if len(unique_values) > 0:\n",
    "            options_value = \";\".join(map(str, unique_values))\n",
    "        else:\n",
    "            options_value = ''\n",
    "        \n",
    "        # fill the options_value in the final DataFrame\n",
    "        df_fin_top_10.at[index, 'options_value'] = options_value\n",
    "    else:\n",
    "        # if the question code does not exist in the 'GSB 2023_V1' DataFrame, fill an empty string\n",
    "        # to be adapted when other version of the answers than 2023 will be available\n",
    "        df_fin_top_10.at[index, 'options_value'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation of the global question using an open source API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libretranslate -> not working locally, instead do : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model to translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection, also tried \"Helsinki-NLP/opus-mt-de-fr\" but the translation was not as good\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "def translate_mbart(text, source_lang, target_lang):\n",
    "    # source language\n",
    "    tokenizer.src_lang = source_lang\n",
    "    # encode\n",
    "    encoded_input = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded_input,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "        max_length=512,\n",
    "        num_beams=4,  # use beam search to improve the translation\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # decode the generated tokens\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10['text_en'] = df_fin_top_10['text_de'].apply(lambda x: translate_mbart(x, \"de_DE\", \"en_XX\"))\n",
    "df_fin_top_10['text_fr'] = df_fin_top_10['text_en'].apply(lambda x: translate_mbart(x, \"en_XX\", \"fr_XX\"))\n",
    "df_fin_top_10['text_it'] = df_fin_top_10['text_en'].apply(lambda x: translate_mbart(x, \"en_XX\", \"it_IT\"))\n",
    "# romanche not available in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10['text_ro'] = 'Not available for the moment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10.to_csv('data/top_10_QuestionGlobales_NLP.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
