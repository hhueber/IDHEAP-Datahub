{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook used to perform NLP on the questions to find some similarities between questions of different surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP-Based Survey Question Grouping and Translation\n",
    "\n",
    "In this notebook, we apply natural language processing to our multi-year survey questions in order to identify semantically similar items and create consistent “global” themes. We:\n",
    "\n",
    "1. **Load and clean question data**  \n",
    "   - Read question metadata from multiple Excel sheets (old vs. new survey years)  \n",
    "   - Apply custom filters to normalize question codes and remove empty entries  \n",
    "\n",
    "2. **Preprocess text**  \n",
    "   - Tokenize, lowercase, remove stop-words and punctuation  \n",
    "   - Lemmatize tokens for consistent word forms  \n",
    "\n",
    "3. **Compute textual similarities**  \n",
    "   - Vectorize questions using TF-IDF  \n",
    "   - Calculate cosine similarity and distance matrices  \n",
    "   - (Optionally) Generate contextual embeddings with BERT/mBART models  \n",
    "\n",
    "4. **Cluster into global themes**  \n",
    "   - Use DBSCAN on similarity distances to group related questions  \n",
    "   - Build a mapping from individual question codes back to their global parent  \n",
    "\n",
    "5. **Translate global labels**  \n",
    "   - Attempt API translation (LibreTranslate)  \n",
    "   - Fall back on Facebook’s mBART model for high-quality multilingual labels  \n",
    "\n",
    "6. **Export results**  \n",
    "   - Save the final `global_questions_nlp.csv`, containing each question code, its assigned global theme, and translated labels  \n",
    "\n",
    "With this pipeline, you’ll have a ready-made table of semantically grouped survey questions—perfect for cross-year comparison and reporting in multiple languages.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests\n",
    "import time\n",
    "from transformers import pipeline\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, MarianMTModel, MarianTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files \n",
    "question_globales_path = 'data/QuestionGlobales.xlsx'\n",
    "extraction_codebook_path = 'data/Extraction CodeBook - 3. Cleaned.xlsx'\n",
    "gsb_path = 'data/GSB 2023_V1.xlsx'\n",
    "\n",
    "# Read the global questions file\n",
    "df_globales = pd.read_excel(question_globales_path)\n",
    "\n",
    "# Read all sheets from the codebook file\n",
    "sheets_codebook = pd.read_excel(extraction_codebook_path, sheet_name=None)\n",
    "\n",
    "# answers dataframe\n",
    "gsb_df = pd.read_excel(gsb_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of data manipulation before NLP preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for each year\n",
    "years = ['1988', '1994', '1998', '2005', '2009', '2017', '2023']\n",
    "dataframes = {}\n",
    "\n",
    "# choose the columns to keep\n",
    "columns_to_keep = ['code', 'num_question', 'year', 'label', 'type', 'format']\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    if year in sheets_codebook:  # ensure the sheet exists\n",
    "        # each sheet is a DataFrame\n",
    "        df = sheets_codebook[year]\n",
    "        # keep only the columns we need (defined in columns_to_keep)\n",
    "        dataframes[f'main_questions_{year}'] = df[columns_to_keep]\n",
    "    else:\n",
    "        print(f\"Sheet for year {year} not found.\")\n",
    "\n",
    "# define the dataframes\n",
    "main_questions_1988 = dataframes.get('main_questions_1988', pd.DataFrame())\n",
    "main_questions_1994 = dataframes.get('main_questions_1994', pd.DataFrame())\n",
    "main_questions_1998 = dataframes.get('main_questions_1998', pd.DataFrame())\n",
    "main_questions_2005 = dataframes.get('main_questions_2005', pd.DataFrame())\n",
    "main_questions_2009 = dataframes.get('main_questions_2009', pd.DataFrame())\n",
    "main_questions_2017 = dataframes.get('main_questions_2017', pd.DataFrame())\n",
    "main_questions_2023 = dataframes.get('main_questions_2023', pd.DataFrame())\n",
    "\n",
    "main_questions_2023.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheet filtering for num_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tailor made data cleaning for the old years sheets \n",
    "def filter_num_question_old(df):\n",
    "    # delete rows with empty num_question\n",
    "    df = df[df['num_question'].notna()].copy()  \n",
    "\n",
    "    # convert num_question to string\n",
    "    df.loc[:, 'num_question'] = df['num_question'].astype(str)  \n",
    "\n",
    "    # some tailor made filtering --> keep only rows with num_question that are digits or contain 'a' or 'A' \n",
    "    df = df[df['num_question'].str.match(r'^\\d+$|.*[aA].*')]\n",
    "    \n",
    "    # keep only the first occurence of each num_question\n",
    "    df = df.drop_duplicates(subset='num_question', keep='first')\n",
    "\n",
    "    # delete the a or A from the num_question\n",
    "    df['num_question'] = df['num_question'].str.replace('a', '', regex=False).str.replace('A', '', regex=False)\n",
    "\n",
    "    # keep only the first occurence of each num_question\n",
    "    df = df.drop_duplicates(subset='num_question', keep='first')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tailor made data cleaning for the new years sheets \n",
    "def filter_num_question_new(df):\n",
    "    # delete rows with empty num_question\n",
    "    df = df[df['num_question'].notna()].copy()  \n",
    "    # keep only the first occurence of each num_question\n",
    "    df = df.drop_duplicates(subset='num_question', keep='first')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_questions_1988 = filter_num_question_old(main_questions_1988)\n",
    "main_questions_1994 = filter_num_question_old(main_questions_1994)\n",
    "main_questions_1998 = filter_num_question_old(main_questions_1998)\n",
    "main_questions_2005 = filter_num_question_old(main_questions_2005)\n",
    "main_questions_2009 = filter_num_question_old(main_questions_2009)\n",
    "main_questions_2017 = filter_num_question_new(main_questions_2017)\n",
    "main_questions_2017 = main_questions_2017[main_questions_2017['num_question'] != 'Q1']\n",
    "main_questions_2023 = filter_num_question_new(main_questions_2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_questions_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_merge = [\n",
    "    main_questions_1988,\n",
    "    main_questions_1994,\n",
    "    main_questions_1998,\n",
    "    main_questions_2005,\n",
    "    main_questions_2009,\n",
    "    main_questions_2017,\n",
    "    main_questions_2023\n",
    "]\n",
    "\n",
    "# merge all the dataframes\n",
    "merged_df = pd.concat(dataframes_to_merge, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. delete punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 3. delete extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 4. lemmatization + tokenization + remove stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop] \n",
    "    \n",
    "    return tokens  \n",
    "\n",
    "# apply the preprocess_text function to the 'label' column\n",
    "tqdm.pandas() \n",
    "merged_df['tokens'] = merged_df['label'].progress_apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['year'] == 2023]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pipeline: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer version: Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dbmdz/bert-base-german-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(labels):\n",
    "    embeddings = []\n",
    "    for label in tqdm(labels, desc='Generating embeddings'):\n",
    "        inputs = tokenizer(label, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()  # convert the list of tokens to a string\n",
    "\n",
    "# obtain the embeddings for the tokens\n",
    "embeddings = get_embeddings(labels)\n",
    "\n",
    "# use cosine similarity to calculate the similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# create a DataFrame from the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=labels, columns=labels)\n",
    "\n",
    "# set the threshold for similarity --> here 0.97 but can be changed\n",
    "threshold = 0.97\n",
    "\n",
    "similar_pairs = []\n",
    "\n",
    "# iterate over the similarity matrix to identify similar pairs\n",
    "for i in tqdm(range(len(labels)), desc='Identifying similar pairs'):\n",
    "    for j in range(i + 1, len(labels)):  \n",
    "        if similarity_matrix[i][j] >= threshold:\n",
    "            similar_pairs.append((labels[i], labels[j]))\n",
    "\n",
    "# create a new DataFrame to store the combined rows\n",
    "question_globale_new = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "for label1, label2 in similar_pairs:\n",
    "    # add a row for each pair of similar tokens\n",
    "    temp_df = merged_df[(merged_df['tokens'].apply(lambda x: ' '.join(x)) == label1) | \n",
    "                        (merged_df['tokens'].apply(lambda x: ' '.join(x)) == label2)]\n",
    "    combined_row = {\n",
    "        'id': id_counter,  # ad id for each combined row\n",
    "        'label': f\"{label1}; {label2}\",\n",
    "        'year': '; '.join(temp_df['year'].astype(str)),\n",
    "        'code': '; '.join(temp_df['code'])\n",
    "    }\n",
    "\n",
    "    question_globale_new = pd.concat([question_globale_new, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "\n",
    "    id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new[question_globale_new['id'] == 1].label.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer version: DSBSAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()\n",
    "\n",
    "# embedding model\n",
    "def get_embeddings(labels):\n",
    "    embeddings = []\n",
    "    for label in tqdm(labels, desc=\"embedding computation\", unit=\"label\"):\n",
    "        inputs = tokenizer(label, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy()  \n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# obtain unique labels\n",
    "labels = merged_df['label'].unique()\n",
    "\n",
    "# obtain embeddings for the labels\n",
    "embeddings = get_embeddings(labels)\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "# use cosine similarity to calculate the similarity matrix\n",
    "distance_matrix = cosine_distances(embeddings)\n",
    "\n",
    "# apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=3, metric='precomputed')  # use precomputed to pass the distance matrix\n",
    "clusters = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "# create a dictionary to map labels to indices\n",
    "label_to_index = {label: index for index, label in enumerate(merged_df['label'].unique())}\n",
    "\n",
    "# create a Series to store the clusters\n",
    "cluster_series = pd.Series(-1, index=merged_df.index)\n",
    "\n",
    "# assign the clusters to the labels\n",
    "for label, cluster_id in zip(labels, clusters):\n",
    "    index = label_to_index[label]\n",
    "    cluster_series.iloc[index] = cluster_id\n",
    "\n",
    "# add the clusters to the merged DataFrame\n",
    "merged_df['cluster'] = cluster_series\n",
    "\n",
    "# create a new DataFrame to store the combined rows\n",
    "question_globale_new = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "# fill the new DataFrame with the combined rows\n",
    "id_counter = 1\n",
    "for cluster_id in set(merged_df['cluster']):\n",
    "    if cluster_id != -1:  \n",
    "        temp_df = merged_df[merged_df['cluster'] == cluster_id]\n",
    "        if not temp_df.empty:\n",
    "            combined_row = {\n",
    "                'id': id_counter,\n",
    "                'label': '; '.join(temp_df['label']),\n",
    "                'year': '; '.join(temp_df['year'].astype(str)),\n",
    "                'code': '; '.join(temp_df['code'])\n",
    "            }\n",
    "            question_globale_new = pd.concat([question_globale_new, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "            id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new[question_globale_new['id'] == 1].label.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 global questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(years):\n",
    "    return len(years) != len(set(years))\n",
    "\n",
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()\n",
    "\n",
    "# create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(labels)\n",
    "\n",
    "# calculate the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "similarity_matrix[np.tril_indices_from(similarity_matrix)] = -1\n",
    "\n",
    "top_10_indices = np.dstack(np.unravel_index(np.argsort(similarity_matrix.ravel())[-10:], similarity_matrix.shape))[0]\n",
    "\n",
    "question_globale_new_top_10 = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "# Traiter les 10 paires ayant la plus grande similarité\n",
    "for i, j in top_10_indices:\n",
    "    label1 = labels[i]\n",
    "    label2 = labels[j]\n",
    "    \n",
    "    # Ajouter une ligne pour chaque paire de tokens similaires\n",
    "    temp_df = merged_df[(merged_df['tokens'].apply(lambda x: ' '.join(x)) == label1) | \n",
    "                        (merged_df['tokens'].apply(lambda x: ' '.join(x)) == label2)]\n",
    "    \n",
    "    combined_years = '; '.join(temp_df['year'].astype(str)).split('; ')\n",
    "\n",
    "    # Vérifier s'il y a des années dupliquées\n",
    "    if has_duplicates(combined_years):\n",
    "        continue  # Ignorer la ligne si des années dupliquées sont trouvées\n",
    "    \n",
    "    # Construire la ligne combinée\n",
    "    combined_row = {\n",
    "        'id': id_counter,  \n",
    "        'label': f\"{label1}; {label2}\",\n",
    "        'year': '; '.join(combined_years),\n",
    "        'code': '; '.join(temp_df['code'])\n",
    "    }\n",
    "\n",
    "    # Ajouter la ligne au DataFrame question_globale_new\n",
    "    question_globale_new_top_10 = pd.concat([question_globale_new_top_10, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "\n",
    "    id_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.loc[question_globale_new_top_10['year'].str.contains('2023'), 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10[question_globale_new_top_10['id'] == 1].label.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question global selection based on threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(years):\n",
    "    return len(years) != len(set(years))\n",
    "\n",
    "# obtain the tokens from the merged DataFrame\n",
    "labels = merged_df['tokens'].apply(lambda x: ' '.join(x)).unique()\n",
    "\n",
    "# create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(labels)\n",
    "\n",
    "# calculate the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# create a DataFrame from the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=labels, columns=labels)\n",
    "\n",
    "# define the threshold for similarity, here 0.8 seems to be a good value but can be changed \n",
    "threshold = 0.6\n",
    "\n",
    "similar_pairs = []\n",
    "\n",
    "# identify similar pairs\n",
    "for i in tqdm(range(len(labels)), desc='Identifying similar pairs'):\n",
    "    for j in range(i + 1, len(labels)):\n",
    "        if similarity_matrix[i][j] >= threshold:\n",
    "            similar_pairs.append((labels[i], labels[j]))\n",
    "\n",
    "# create a new DataFrame to store the combined rows\n",
    "question_globale_new_threshold = pd.DataFrame(columns=['id', 'label', 'year', 'code'])\n",
    "\n",
    "id_counter = 1\n",
    "\n",
    "for label1, label2 in similar_pairs:\n",
    "    # add a row for each pair of similar tokens\n",
    "    temp_df = merged_df[(merged_df['tokens'].apply(lambda x: ' '.join(x)) == label1) | \n",
    "                        (merged_df['tokens'].apply(lambda x: ' '.join(x)) == label2)]\n",
    "    \n",
    "    combined_years = '; '.join(temp_df['year'].astype(str)).split('; ')\n",
    "\n",
    "    if has_duplicates(combined_years):\n",
    "        continue\n",
    "    \n",
    "    combined_row = {\n",
    "        'id': id_counter,  \n",
    "        'label': f\"{label1}; {label2}\",\n",
    "        'year': '; '.join(combined_years),\n",
    "        'code': '; '.join(temp_df['code'])\n",
    "    }\n",
    "\n",
    "    question_globale_new_threshold = pd.concat([question_globale_new_threshold, pd.DataFrame([combined_row])], ignore_index=True)\n",
    "\n",
    "    id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_threshold.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the results seems better --> use the TF-IDF version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV extraction of the global questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_glob_columns = [\n",
    "    'label', 'code_first_question','code_other_question','text_de', 'text_fr', 'text_it', 'text_ro', 'text_en',\n",
    "    'category_label', 'category_text_de', 'category_text_fr',\n",
    "    'category_text_it', 'category_text_ro', 'category_text_en',\n",
    "    'options_value', 'options_label'\n",
    "]\n",
    "\n",
    "code_to_token = {}\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    code = row['code']\n",
    "    tokens = row['tokens']\n",
    "\n",
    "    for token in tokens:\n",
    "        if not token.isdigit():  \n",
    "            if code not in code_to_token:  \n",
    "                code_to_token[code] = token\n",
    "            break\n",
    "\n",
    "\n",
    "print(code_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_globale_new_top_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame to store the final results\n",
    "df_fin_top_10 = pd.DataFrame(columns=quest_glob_columns)\n",
    "\n",
    "# fill the new DataFrame with the combined rows\n",
    "for index, row in question_globale_new_top_10.iterrows():\n",
    "    codes_list = row['code'].split('; ')\n",
    "    gsb23_code = next((code for code in row['code'].split('; ') if code.startswith('GSB23_')), None)\n",
    "    code_first_question = gsb23_code if gsb23_code is not None else row['code'].split('; ')[0]  # use 'GSB23_' code or the first code\n",
    "    code_other_question = '; '.join([code for code in codes_list if code != code_first_question])\n",
    "    first_token = code_to_token.get(code_first_question, '')  # use the token corresponding to the first code\n",
    "\n",
    "    # Find the corresponding text_de in merged_df using code_first_question\n",
    "    label_from_merged = merged_df.loc[merged_df['code'] == code_first_question, 'label'].values\n",
    "    if len(label_from_merged) > 0:\n",
    "        # Remove any leading numbers and dots (e.g., \"52. \" or \"123. \")\n",
    "        cleaned_label = re.sub(r'^\\d+\\.\\s*', '', label_from_merged[0])\n",
    "        text_de_first_question = cleaned_label\n",
    "    else:\n",
    "        text_de_first_question = row['label'].split('; ')[0]\n",
    "\n",
    "    # create a new row with the required columns\n",
    "    new_row = {\n",
    "        'label': first_token,  # use the first code in 'label'\n",
    "        'code_first_question': code_first_question,  # put the first code in 'code_first_question'\n",
    "        'code_other_question': code_other_question,  # put the other codes in 'code_other_question'\n",
    "        'text_de': text_de_first_question,  # use the first label in 'text_de'\n",
    "        'text_fr': '',  \n",
    "        'text_it': '', \n",
    "        'text_ro': '', \n",
    "        'text_en': '',\n",
    "        'category_label': '',\n",
    "        'category_text_de': '',\n",
    "        'category_text_fr': '',\n",
    "        'category_text_it'  : '',\n",
    "        'category_text_ro': '',\n",
    "        'category_text_en': '',\n",
    "        'options_value': '',\n",
    "        'options_label': ''\n",
    "    }\n",
    "\n",
    "    # add the new row to the final DataFrame\n",
    "    df_fin_top_10 = pd.concat([df_fin_top_10, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "\n",
    "df_fin_top_10.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsb_columns = gsb_df.columns\n",
    "\n",
    "# fill the 'options_value' column in df_fin\n",
    "for index, row in df_fin_top_10.iterrows():\n",
    "    # get the code of the first question\n",
    "    question_code = row['code_first_question']\n",
    "    \n",
    "    # check if the question code exists in the 'GSB 2023_V1' DataFrame\n",
    "    if question_code in gsb_columns:\n",
    "        # extract the unique values from the column\n",
    "        unique_values = gsb_df[question_code].dropna().unique()\n",
    "        \n",
    "        # if there are unique values, join them \n",
    "        if len(unique_values) > 0:\n",
    "            options_value = \";\".join(map(str, unique_values))\n",
    "        else:\n",
    "            options_value = ''\n",
    "        \n",
    "        # fill the options_value in the final DataFrame\n",
    "        df_fin_top_10.at[index, 'options_value'] = options_value\n",
    "    else:\n",
    "        # if the question code does not exist in the 'GSB 2023_V1' DataFrame, fill an empty string\n",
    "        # to be adapted when other version of the answers than 2023 will be available\n",
    "        df_fin_top_10.at[index, 'options_value'] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation of the global question using an open source API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libretranslate -> not working locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model to translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model selection, also tried \"Helsinki-NLP/opus-mt-de-fr\" but the translation was not as good\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "def translate_mbart(text, source_lang, target_lang):\n",
    "    # source language\n",
    "    tokenizer.src_lang = source_lang\n",
    "    # encode\n",
    "    encoded_input = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded_input,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "        max_length=512,\n",
    "        num_beams=4,  # use beam search to improve the translation\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # decode the generated tokens\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10['text_en'] = df_fin_top_10['text_de'].apply(lambda x: translate_mbart(x, \"de_DE\", \"en_XX\"))\n",
    "df_fin_top_10['text_fr'] = df_fin_top_10['text_en'].apply(lambda x: translate_mbart(x, \"en_XX\", \"fr_XX\"))\n",
    "df_fin_top_10['text_it'] = df_fin_top_10['text_en'].apply(lambda x: translate_mbart(x, \"en_XX\", \"it_IT\"))\n",
    "# romanche not available in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10['text_ro'] = 'Not available for the moment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin_top_10.to_csv('data/top_10_QuestionGlobales_NLP.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
