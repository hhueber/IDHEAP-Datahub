{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching GSB23 Survey Data with Geospatial Mapping & Multilingual Labels\n",
    "\n",
    "In this notebook, we build on our combined 2023 survey dataset to:\n",
    "\n",
    "1. **Map municipality responses**  \n",
    "   - Load municipal boundaries from `municipalities.json`  \n",
    "   - Match each respondent’s commune ID to its GeoJSON feature  \n",
    "   - Embed survey answers into the GeoJSON properties  \n",
    "   - Export `commune_responses.csv` for downstream mapping\n",
    "\n",
    "2. **Generate human-readable translations**  \n",
    "   - Clean question labels by removing leading numbers  \n",
    "   - Use Facebook’s mBART model to translate German labels into English  \n",
    "   - Cascade translations from English → French → Italian  \n",
    "   - Mark Romanche translations as “not available” for now\n",
    "\n",
    "3. **Export the final enriched dataset**  \n",
    "   - Save the fully geocoded, multilingual DataFrame to `data/combined_df.csv`  \n",
    "\n",
    "With these steps, you’ll have a ready-to-use file for both geospatial visualization and multilingual reporting.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_codebook_23 = pd.read_excel(\"data/Extraction CodeBook - 3. Cleaned.xlsx\", sheet_name=\"2023\")\n",
    "df_gsb_23 = pd.read_excel(\"data/GSB 2023_V1.xlsx\")\n",
    "df_qg = pd.read_excel(\"data/QuestionGlobales.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codebook_23.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gsb_23.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_gsb_23.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gsb_23.head(4)\n",
    "df_gsb_23[df_gsb_23[\"BFS_2023\"] == 5586]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gsb_23.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data merging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_questions = df_codebook_23[df_codebook_23['code'].str.contains(r'GSB23_Q\\d+\\b')]\n",
    "base_question_ids = df_base_questions['code'].str.extract(r'GSB23_Q(\\d+)')[0].astype(int).unique()\n",
    "df_suffix_questions = df_codebook_23[df_codebook_23['code'].str.contains(r'GSB23_Q\\d+_1\\b')]\n",
    "for question_id in base_question_ids:\n",
    "    df_suffix_questions = df_suffix_questions[~df_suffix_questions['code'].str.contains(f'GSB23_Q{question_id}_1\\b')]\n",
    "df_first_subquestions = pd.concat([df_base_questions, df_suffix_questions], ignore_index=True)\n",
    "df_first_subquestions.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_first_subquestions[df_first_subquestions['num_question'] == 'Q1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_subquestions[df_first_subquestions[\"enquete\"] == \"GSB23\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_subquestions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Liste des questions supplémentaires à ajouter\n",
    "extra_questions_codes = [\n",
    "    'GSB23_Teilnahme', 'GSB23_Mode', 'GSB23_StartDate', 'GSB23_EndDate', \n",
    "    'GSB23_Progress', 'GSB23_Duration', 'GSB23_Finished', 'GSB23_Recorded', \n",
    "    'GSB23_UserLang', 'GSB23_Q99', 'GSB23_Q100', 'GSB23_Q101'\n",
    "]\n",
    "\n",
    "# Step 2: Filtrer le df_codebook pour ces questions\n",
    "df_extra_questions = df_codebook_23[df_codebook_23['code'].isin(extra_questions_codes)]\n",
    "\n",
    "# Step 3: Combiner les 10 premières sous-questions avec les questions supplémentaires\n",
    "df_combined = pd.concat([df_first_subquestions, df_extra_questions])\n",
    "\n",
    "# Step 4: Réinitialiser les index du DataFrame combiné\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 5: Afficher le DataFrame combiné\n",
    "df_combined.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('municipalities.json', encoding=\"utf-8\") as f:\n",
    "    municipalities_data = json.load(f)\n",
    "\n",
    "list_of_commune_ids = [feature['properties']['id'] for feature in municipalities_data['features']]\n",
    "#print(list_of_commune_ids)\n",
    "\n",
    "# the column GSB23_Q100 contains the commune ids\n",
    "df_filtered = df_gsb_23[df_gsb_23['GSB23_Q100'].isin(list_of_commune_ids)]\n",
    "\n",
    "col_of_interest = df_combined['code_original'].tolist()\n",
    "\n",
    "print(col_of_interest)\n",
    "# Filtrer seulement les colonnes qui existent dans df_filtered\n",
    "col_of_interest_filtered = [col for col in col_of_interest if col in df_filtered.columns]\n",
    "\n",
    "df_commune_responses = df_filtered[col_of_interest_filtered]\n",
    "df_commune_responses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commune_responses = df_commune_responses.loc[:, ~df_commune_responses.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_commune_responses.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in municipalities_data['features']:\n",
    "    commune_id = feature['properties']['id']\n",
    "    \n",
    "    # Filtrer les réponses pour cette commune\n",
    "    commune_response = df_commune_responses[df_commune_responses['GSB23_Q100'] == commune_id]\n",
    "    \n",
    "    if not commune_response.empty:\n",
    "        # Ajouter la réponse au GeoJSON (par exemple pour une question)\n",
    "        feature['properties']['response_Q1'] = commune_response['GSB23_Q1_1'].values[0]\n",
    "\n",
    "df_commune_responses.to_csv('data/commune_responses.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "df_combined['text_de'] = df_combined['label'].str.replace(r'^\\d+\\.\\s*', '', regex=True)\n",
    "\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# translation function with mbart \n",
    "def translate_mbart(text, source_lang, target_lang):\n",
    "    tokenizer.src_lang = source_lang\n",
    "    # encode the text to be translated\n",
    "    encoded_input = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded_input,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "        max_length=512,\n",
    "        num_beams=4,  # use beam search for better results\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # decode the generated tokens\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "df_combined['text_en'] = df_combined['text_de'].apply(lambda x: translate_mbart(x, \"de_DE\", \"en_XX\"))\n",
    "df_combined['text_fr'] = df_combined['text_en'].apply(lambda x: translate_mbart(x, \"en_XX\", \"fr_XX\"))\n",
    "df_combined['text_it'] = df_combined['text_en'].apply(lambda x: translate_mbart(x, \"en_XX\", \"it_IT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['text_ro'] = 'Translation not available for the moment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('data/combined_df.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
